{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.fft import fft, fftfreq\n",
    "import scipy.signal as signal\n",
    "import imageio\n",
    "import sys\n",
    "from scipy.signal import butter, lfilter, get_window, medfilt\n",
    "import pywt\n",
    "import seaborn as sns\n",
    "from scipy.stats import spearmanr, pearsonr\n",
    "from sklearn.decomposition import PCA\n",
    "import statsmodels.graphics.tsaplots as tsaplots\n",
    "import random\n",
    "import scipy.stats\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from scipy.optimize import minimize\n",
    "\n",
    "\n",
    "import sys\n",
    "from scipy.signal import savgol_filter, medfilt, find_peaks\n",
    "from functools import partial\n",
    "\n",
    "sys.path.append(\"../\")\n",
    "sys.path.append(\"../scripts\")\n",
    "\n",
    "from file_io import read_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "gt_df = pd.read_csv(\"../data/raw/train_labels.csv\")\n",
    "target_star_list = gt_df.planet_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Required functions\n",
    "def smooth_data(data, window_size):\n",
    "    return savgol_filter(data, window_size, 3)  # window size 51, polynomial order 3\n",
    "\n",
    "\n",
    "def optimize_breakpoint(\n",
    "    data, initial_breakpoint=50, window_size=20, buffer_size=5, smooth_window=5\n",
    "):\n",
    "    \"\"\"\n",
    "    input: 1D time series; first breakpoint guess\n",
    "    output: first breakpoint\n",
    "    \"\"\"\n",
    "    best_breakpoint = initial_breakpoint\n",
    "    best_score = float(\"-inf\")\n",
    "    midpoint = len(data) // 2\n",
    "    smoothed_data = smooth_data(data, smooth_window)\n",
    "\n",
    "    data_len = len(data)\n",
    "    for i in range(-window_size, window_size):\n",
    "        new_breakpoint = initial_breakpoint + i\n",
    "        if new_breakpoint - buffer_size < 0 or new_breakpoint + buffer_size >= data_len:\n",
    "            continue\n",
    "\n",
    "        region1 = data[: new_breakpoint - buffer_size]\n",
    "        region2 = data[\n",
    "            new_breakpoint + buffer_size : data_len - new_breakpoint - buffer_size\n",
    "        ]\n",
    "        region3 = data[data_len - new_breakpoint + buffer_size :]\n",
    "\n",
    "        if len(region1) == 0 or len(region2) == 0 or len(region3) == 0:\n",
    "            continue\n",
    "\n",
    "        # calc on smoothed data\n",
    "        breakpoint_region1 = smoothed_data[\n",
    "            new_breakpoint - buffer_size : new_breakpoint + buffer_size\n",
    "        ]\n",
    "        breakpoint_region2 = smoothed_data[\n",
    "            -(new_breakpoint + buffer_size) : -(new_breakpoint - buffer_size)\n",
    "        ]\n",
    "\n",
    "        mean_diff = abs(np.mean(region1) - np.mean(region2)) + abs(\n",
    "            np.mean(region2) - np.mean(region3)\n",
    "        )\n",
    "        var_sum = np.var(region1) + np.var(region2) + np.var(region3)\n",
    "\n",
    "        range_at_breakpoint1 = np.ptp(\n",
    "            breakpoint_region1\n",
    "        )  # ptp: peak to peak (max - min)\n",
    "        range_at_breakpoint2 = np.ptp(breakpoint_region2)\n",
    "\n",
    "        mean_range_at_breakpoint = (range_at_breakpoint1 + range_at_breakpoint2) / 2\n",
    "\n",
    "        score = mean_diff - 0.5 * var_sum + mean_range_at_breakpoint\n",
    "\n",
    "        if score > best_score:\n",
    "            best_score = score\n",
    "            best_breakpoint = new_breakpoint\n",
    "\n",
    "    return best_breakpoint, data_len - best_breakpoint\n",
    "\n",
    "\n",
    "def try_s(signal, p1, p2, deg, s):\n",
    "    out = np.concatenate((np.arange(p1 - 10), np.arange(p2 + 10, signal.shape[0])))\n",
    "    x = np.concatenate((out, np.arange(p1, p2)))\n",
    "    y = np.concatenate((signal[out], signal[p1:p2] * (1 / (1 - s[0]))))\n",
    "\n",
    "    z = np.polyfit(x, y, deg)\n",
    "    p = np.poly1d(z)\n",
    "    q = np.sqrt(np.mean((p(x) - y) ** 2))\n",
    "\n",
    "    if s < 1e-4:\n",
    "        return q + 1e3\n",
    "\n",
    "    return q\n",
    "\n",
    "\n",
    "def calibrate_signal(signal, ingress, egress, full_output=False):\n",
    "    \"\"\"\n",
    "    input: 1D time series\n",
    "    output: avg absorption (scalar), time stamp, raw time series, predicted time series\n",
    "    \"\"\"\n",
    "\n",
    "    p1, p2 = ingress, egress\n",
    "\n",
    "    best_deg, best_score, best_s = 1, 1e12, None\n",
    "    out = np.concatenate((np.arange(p1 - 10), np.arange(p2 + 10, signal.shape[0])))\n",
    "    x_out = out\n",
    "    x_in = np.arange(p1, p2)\n",
    "    x = np.concatenate((x_out, x_in))\n",
    "\n",
    "    for deg in range(1, 6):\n",
    "        f = partial(try_s, signal, p1, p2, deg)\n",
    "        r = minimize(f, [0.001], method=\"Nelder-Mead\")\n",
    "        s = r.x[0]\n",
    "\n",
    "        y = np.concatenate((signal[out], signal[p1:p2] * (1 / (1 - s))))\n",
    "\n",
    "        z = np.polyfit(x, y, deg)\n",
    "        p = np.poly1d(z)\n",
    "        q = np.sqrt(np.mean((p(x) - y) ** 2))\n",
    "\n",
    "        if q < best_score:\n",
    "            best_score = q\n",
    "            best_deg = deg\n",
    "            best_s = s\n",
    "\n",
    "    # x = np.concatenate((x_out, x_in))\n",
    "    y = np.concatenate((signal[out], signal[p1:p2] * (1 / (1 - best_s))))\n",
    "    z = np.polyfit(x, y, best_deg)\n",
    "    p = np.poly1d(z)\n",
    "\n",
    "    if full_output:\n",
    "        return best_s, x, y, p(x), p\n",
    "    else:\n",
    "        return signal - p(np.arange(signal.shape[0])) + signal[0]\n",
    "\n",
    "\n",
    "def ratio_from_timeseries(signal, phase_0, phase_1, buffer=5, est_mean=False):\n",
    "    \"\"\"\n",
    "    input: 1D time series; phase_0 (start of eclipse); phase_1 (end of eclipse)\n",
    "    output: predicted absorption ratio\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    # out of transit average flux intensiy\n",
    "    oot_flux = np.concat((signal[: phase_0 - buffer], signal[phase_1 + buffer :]))\n",
    "    it_flux = signal[phase_0 + buffer : phase_1 - buffer]\n",
    "\n",
    "    # avg intensity\n",
    "    oot_avg = np.mean(oot_flux)\n",
    "    it_avg = np.mean(it_flux)\n",
    "\n",
    "    # estimated noise\n",
    "    oot_var = np.std(oot_flux)\n",
    "    it_var = np.std(it_flux)\n",
    "\n",
    "    # calculate weighing\n",
    "    oot_weight = 1 / np.max((oot_var, 1e-5))\n",
    "    it_weight = 1 / np.max((it_var, 1e-5))\n",
    "    obs_weight = np.mean((oot_weight, it_weight))\n",
    "\n",
    "    # if np.isnan(oot_weight).sum() + np.isnan(it_weight).sum() > 0:\n",
    "    # print(\"error\", oot_weight, it_weight, it_flux, phase_0, phase_1, signal[])\n",
    "    # print(\"ok\")\n",
    "    obs_ratio = np.clip((oot_avg - it_avg) / (it_avg + 1e-6), 0, None)\n",
    "\n",
    "    # print(oot_avg, it_avg, obs_ratio)\n",
    "    if est_mean:\n",
    "        est_ratio = obs_weight * obs_ratio + (1 - obs_weight) * est_mean\n",
    "        return est_ratio\n",
    "\n",
    "    else:\n",
    "        est_ratio = obs_ratio\n",
    "        return est_ratio\n",
    "\n",
    "\n",
    "def calculate_spectrum(lc, wc, lc_phases, wc_phases):\n",
    "    \"\"\"\n",
    "    input:\n",
    "    -- lc: obs * t * freq\n",
    "    -- wc: obs * t\n",
    "    -- lc_phases: obs * freq * 2 (1 for start 1 for end of eclipse)\n",
    "    -- wc_phases: obs * 2 (1 for start 1 for end of eclipse)\n",
    "\n",
    "    output\n",
    "    -- lc_ratio: obs * freq\n",
    "    -- wc_ratio: obs * freq (flat line across all freq; this shows the avg pred)\n",
    "    \"\"\"\n",
    "\n",
    "    lc_ratio = np.zeros((lc.shape[0], lc.shape[2]))\n",
    "    wc_ratio = np.zeros((wc.shape[0], lc.shape[2]))\n",
    "\n",
    "    for i in range(len(lc)):\n",
    "\n",
    "        for f in range(lc.shape[2]):\n",
    "            freq_timeseries = lc[i, :, f]\n",
    "            freq_ratio = ratio_from_timeseries(\n",
    "                freq_timeseries, lc_phases[i][f][0], lc_phases[i][f][1]\n",
    "            )\n",
    "            lc_ratio[i, f] = freq_ratio\n",
    "\n",
    "        wc_ratio[i] = ratio_from_timeseries(wc[i], wc_phases[i][0], wc_phases[i][1])\n",
    "\n",
    "    return lc_ratio, wc_ratio\n",
    "\n",
    "\n",
    "def subtract_background(timeseries, edge_pixel=5):\n",
    "    \"\"\"\n",
    "    input: observations * t * freq * spatial\n",
    "    output: observations * t * freq * spatial\n",
    "\n",
    "    \"\"\"\n",
    "    # compute average of the edge pixel, treat them as background noise\n",
    "    first_5_rows = timeseries[:, :, :, :edge_pixel]\n",
    "    last_5_rows = timeseries[:, :, :, -edge_pixel:]\n",
    "\n",
    "    background_noise = (\n",
    "        np.mean(first_5_rows, axis=3) + np.mean(last_5_rows, axis=3)\n",
    "    ) / 2\n",
    "\n",
    "    print(background_noise.shape)\n",
    "\n",
    "    # Step 3: Subtract the background noise from all rows along axis 2\n",
    "    # Use broadcasting to subtract it from each row of the 3rd axis\n",
    "    data_corrected = timeseries - background_noise[:, :, :, np.newaxis]\n",
    "\n",
    "    return data_corrected\n",
    "\n",
    "\n",
    "def spatial_integration(timeseries, range_start=8, range_end=24):\n",
    "    \"\"\"\n",
    "    input: observation * t * freq * spatial\n",
    "    output: observation * t * freq\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    timeseries = timeseries[:, :, :, range_start:range_end].sum(axis=3)\n",
    "\n",
    "    return timeseries\n",
    "\n",
    "\n",
    "def bin_frequencies(time_series, k):\n",
    "    \"\"\"\n",
    "    Bin the frequencies of a time series such that each bin has roughly equal intensity.\n",
    "\n",
    "    Parameters:\n",
    "    - time_series: numpy.ndarray, the time series data of shape (observations * time_steps, frequency).\n",
    "    - k: int, the number of bins.\n",
    "\n",
    "    Returns:\n",
    "    - binned_series: numpy.ndarray, the binned time series data of shape (time_steps, k).\n",
    "    \"\"\"\n",
    "\n",
    "    full_binned_series = np.zeros_like(time_series)\n",
    "    # full_binned_series = np.zeros((time_series.shape[0], time_series.shape[1], k))\n",
    "\n",
    "    for i in range(time_series.shape[0]):\n",
    "        # Compute the total intensity for each frequency\n",
    "        total_intensity = np.sum(time_series[i], axis=0)\n",
    "\n",
    "        # Compute the cumulative intensity\n",
    "        cumulative_intensity = np.cumsum(total_intensity)\n",
    "\n",
    "        # Determine the total intensity and the target intensity per bin\n",
    "        total_intensity_sum = cumulative_intensity[-1]\n",
    "        target_intensity_per_bin = total_intensity_sum / k\n",
    "\n",
    "        # Initialize the binned series\n",
    "        time_steps, freq = time_series[i].shape\n",
    "        binned_series = np.zeros((time_steps, k))\n",
    "\n",
    "        # Bin the frequencies\n",
    "        bin_idx = 0\n",
    "        bin_intensity = 0\n",
    "        bin_indices = [0]\n",
    "        for j in range(freq):\n",
    "            if (\n",
    "                bin_intensity + total_intensity[j] > target_intensity_per_bin\n",
    "                and bin_idx < k - 1\n",
    "            ):\n",
    "                bin_indices.append(j)\n",
    "                bin_idx += 1\n",
    "                bin_intensity = 0\n",
    "            binned_series[:, bin_idx] += time_series[i, :, j]\n",
    "            bin_intensity += total_intensity[j]\n",
    "        bin_indices.append(j + 1)\n",
    "        # print(bin_indices)\n",
    "        # print(binned_series.shape)\n",
    "        for bin_index in range(len(bin_indices) - 1):\n",
    "            full_binned_series[i][\n",
    "                :, bin_indices[bin_index] : bin_indices[bin_index + 1]\n",
    "            ] = binned_series[:, bin_index].reshape(-1, 1)\n",
    "    return full_binned_series\n",
    "\n",
    "\n",
    "def detector_runner(timeseries, detector_method):\n",
    "    \"\"\"\n",
    "    input: # obs * t * freq\n",
    "    output:\n",
    "        -- LC phases: # obs * freq * 2\n",
    "        -- WC phases: # obs * 2\n",
    "    \"\"\"\n",
    "\n",
    "    lc_phases = np.zeros((timeseries.shape[0], timeseries.shape[2], 2), dtype=int)\n",
    "    wc_phases = np.zeros((timeseries.shape[0], 2), dtype=int)\n",
    "\n",
    "    # wc_timeseries = timeseries.sum(axis=2)\n",
    "\n",
    "    for i, observation in enumerate(timeseries):\n",
    "        for freq in range(len(observation[0])):\n",
    "            lc_phases[i, freq, 0], lc_phases[i, freq, 1] = detector_method(\n",
    "                observation[:, freq]\n",
    "            )\n",
    "        wc_phases[i, 0], wc_phases[i, 1] = detector_method(observation.sum(axis=(1)))\n",
    "\n",
    "    return lc_phases, wc_phases\n",
    "\n",
    "\n",
    "def model_runner(timeseries, lc_phases, wc_phases, method):\n",
    "    \"\"\"\n",
    "    input:\n",
    "    -- timeseries: obs * t * freq\n",
    "    -- lc_phases: obs * freq * 2\n",
    "    -- wc_phases: obs * 2\n",
    "\n",
    "    output:\n",
    "    -- fitted_lc: obs * t * freq\n",
    "    -- fitted_wc: obs * t\n",
    "    -- lc_flux_ratio: obs * freq\n",
    "    -- wc_flux_ratio: obs\n",
    "    \"\"\"\n",
    "\n",
    "    fitted_lc = np.zeros(timeseries.shape, dtype=int)\n",
    "    fitted_wc = np.zeros((timeseries.shape[0], timeseries.shape[1]), dtype=int)\n",
    "\n",
    "    for i, observation in enumerate(timeseries):\n",
    "        for j in range(timeseries.shape[2]):\n",
    "            lc_phase_1 = lc_phases[i, j, 0]\n",
    "            lc_phase_2 = lc_phases[i, j, 1]\n",
    "\n",
    "            # print(\"testing\", lc_phase_1, lc_phase_2)\n",
    "            # print(timeseries[i, :, j])\n",
    "            fitted_lc[i, :, j] = method(\n",
    "                timeseries[i, :, j], lc_phase_1 + 5, lc_phase_2 - 5\n",
    "            )  # include buffer here\n",
    "        wc_phase_1 = wc_phases[i, 0] + 5  # include buffer here\n",
    "        wc_phase_2 = wc_phases[i, 1] - 5  # include buffer here\n",
    "\n",
    "        fitted_wc[i] = method(\n",
    "            signal.medfilt(timeseries[i].sum(axis=1), 5), wc_phase_1, wc_phase_2\n",
    "        )\n",
    "\n",
    "    return fitted_lc, fitted_wc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'target_star_list' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Prediction sequence\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m selected_targets \u001b[38;5;241m=\u001b[39m \u001b[43mtarget_star_list\u001b[49m[:\u001b[38;5;241m2\u001b[39m]\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mprediction_logic\u001b[39m(target_list, gt_df):\n\u001b[1;32m      6\u001b[0m \n\u001b[1;32m      7\u001b[0m     \u001b[38;5;66;03m# read files\u001b[39;00m\n\u001b[1;32m      8\u001b[0m     airs_selected \u001b[38;5;241m=\u001b[39m []\n",
      "\u001b[0;31mNameError\u001b[0m: name 'target_star_list' is not defined"
     ]
    }
   ],
   "source": [
    "# Prediction sequence\n",
    "# selected_targets = target_star_list[:2]\n",
    "\n",
    "\n",
    "def prediction_logic():\n",
    "\n",
    "    # read files\n",
    "    airs_selected = []\n",
    "    fgs_selected = []\n",
    "    # gt_selected = []\n",
    "\n",
    "    # for target in target_list:\n",
    "    #     # airs, fgs, gt = read_files(target, gt_df)\n",
    "    #     airs = np.load(f\"../data/processed/{target_star}_airs.npy\")\n",
    "    #     fgs = np.load(f\"../data/processed/{target_star}_fgs1.npy\")\n",
    "    #     airs_selected.append(airs)\n",
    "    #     fgs_selected.append(fgs)\n",
    "    #     # gt_selected.append(gt)\n",
    "\n",
    "    airs_selected = np.load(\"airs_v4.npy\")\n",
    "    # fgs_selected = np.vstack(fgs_selected)\n",
    "    # gt_selected = np.vstack(gt_selected)\n",
    "    print(\"file read\")\n",
    "\n",
    "    # PREPROCESS\n",
    "    # 1. subtracting the estimated background signal;\n",
    "    airs_selected = subtract_background(airs_selected)\n",
    "    # 2. integrate spatially across the center of the detector. Output shape of observations * t * freq\n",
    "    airs_selected = spatial_integration(airs_selected)\n",
    "\n",
    "    # 3. additional smoothing step\n",
    "    # airs_selected = wavelet_smoothing(airs_selected, 5) # wavelet smoothing seems to decrease the max-min range of signal, we will skip this for now\n",
    "\n",
    "    # DE-TREND\n",
    "    # phase detection - returns the estimated instants at which entry and exit occurs for each eclipse event\n",
    "    lc_phases, wc_phases = detector_runner(airs_selected, optimize_breakpoint)\n",
    "\n",
    "    lc_phases = np.tile(\n",
    "        wc_phases[:, np.newaxis, :], (1, 282, 1)\n",
    "    )  # we will use wc_phases for both lc and wc for the time being, due to resolution limits\n",
    "\n",
    "    # Frequency Binning:\n",
    "    binned_airs_selected = bin_frequencies(airs_selected, 10)\n",
    "\n",
    "    fitted_lc, fitted_wc = model_runner(\n",
    "        binned_airs_selected, lc_phases, wc_phases, calibrate_signal\n",
    "    )  # added median smoothing to the model runner\n",
    "\n",
    "    # Cal prediction\n",
    "    lc_ratio, wc_ratio = calculate_spectrum(fitted_lc, fitted_wc, lc_phases, wc_phases)\n",
    "\n",
    "    return (\n",
    "        airs_selected,\n",
    "        lc_phases,\n",
    "        wc_phases,\n",
    "        fitted_lc,\n",
    "        fitted_wc,\n",
    "        # gt_selected,\n",
    "        lc_ratio,\n",
    "        wc_ratio,\n",
    "    )\n",
    "    # print(airs_selected.shape)\n",
    "\n",
    "\n",
    "(\n",
    "    airs_selected,\n",
    "    lc_phases,\n",
    "    wc_phases,\n",
    "    fitted_lc,\n",
    "    fitted_wc,\n",
    "    # gt_selected,\n",
    "    lc_ratio,\n",
    "    wc_ratio,\n",
    ") = prediction_logic()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1025,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_sub = pd.read_csv(\"../data/raw/sample_submission.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1154,
   "metadata": {},
   "outputs": [],
   "source": [
    "gt_df = pd.read_csv(\"../data/raw/train_labels.csv\")\n",
    "del gt_df[\"planet_id\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1165,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0 0.3938202007381944\n",
      "1.01 0.39445602621175563\n",
      "1.02 0.395039057252732\n",
      "1.03 0.3955716660641241\n",
      "1.04 0.396056104968149\n",
      "1.05 0.3964945134061458\n",
      "1.06 0.3968889244715333\n",
      "1.07 0.3972412710108922\n",
      "1.08 0.3975533913253136\n",
      "1.09 0.39782703450150475\n",
      "1.1 0.39806386539970634\n",
      "1.11 0.3982654693233\n",
      "1.12 0.39843335639296773\n",
      "1.1300000000000001 0.39856896564644734\n",
      "1.1400000000000001 0.3986736688832508\n",
      "1.1500000000000001 0.39874877427221167\n",
      "1.1600000000000001 0.39879552973831683\n",
      "1.1700000000000002 0.3988151261440305\n",
      "1.1800000000000002 0.39880870027912574\n",
      "1.1900000000000002 0.3987773376720115\n",
      "1.2000000000000002 0.3987220752345316\n",
      "1.2100000000000002 0.398643903751339\n",
      "1.2200000000000002 0.3985437702241226\n",
      "1.2300000000000002 0.39842258008020054\n",
      "1.2400000000000002 0.39828119925430905\n",
      "1.2500000000000002 0.3981204561517799\n",
      "1.2600000000000002 0.3979411435006977\n",
      "1.2700000000000002 0.3977440201001011\n",
      "1.2800000000000002 0.3975298124707928\n",
      "1.2900000000000003 0.39729921641484883\n",
      "1.3000000000000003 0.3970528984895105\n",
      "1.3100000000000003 0.3967914974007431\n",
      "1.3200000000000003 0.3965156253213748\n",
      "1.3300000000000003 0.39622586913841523\n",
      "1.3400000000000003 0.3959227916338148\n",
      "1.3500000000000003 0.39560693260267044\n",
      "1.3600000000000003 0.39527880991258735\n",
      "1.3700000000000003 0.3949389205076877\n",
      "1.3800000000000003 0.3945877413605012\n",
      "1.3900000000000003 0.3942257303747893\n",
      "1.4000000000000004 0.39385332724213484\n",
      "1.4100000000000004 0.39347095425495915\n",
      "1.4200000000000004 0.3930790170784498\n",
      "1.4300000000000004 0.3926779054837416\n",
      "1.4400000000000004 0.39226799404451457\n",
      "1.4500000000000004 0.3918496427990815\n",
      "1.4600000000000004 0.39142319787986013\n",
      "1.4700000000000004 0.3909889921120485\n",
      "1.4800000000000004 0.3905473455831875\n",
      "1.4900000000000004 0.39009856618519567\n"
     ]
    }
   ],
   "source": [
    "preds = lc_ratio[:, ::-1]\n",
    "constant_value = np.full((preds.shape[0], 1), lc_ratio.mean())\n",
    "preds = np.hstack((constant_value, preds))\n",
    "\n",
    "sigmas = np.vstack(\n",
    "    [np.std((lc_ratio[:, ::-1] - gt_df.iloc[:, 1:].values), axis=0) * i]\n",
    "    * lc_ratio.shape[0]\n",
    ")\n",
    "constant_value = np.full((sigmas.shape[0], 1), sigmas.mean())\n",
    "sigmas = np.hstack((constant_value, sigmas))\n",
    "\n",
    "sub_frame = pd.DataFrame(\n",
    "    np.concatenate([preds, sigmas], axis=1), columns=sample_sub.columns[1:]\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "KaggleAriel",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
